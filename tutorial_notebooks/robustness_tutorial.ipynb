{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Robustness Tutorial\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "- Implement and run adversarial attacks on a standard neural network.\n",
    "- Build and train a Lipschitz-constrained network (LipNet).\n",
    "- Evaluate the empirical and certified robustness of the LipNet.\n",
    "\n",
    "**Table of contents**\n",
    "\n",
    "1. [üß† Train a standard neural network on MNIST](#train-mnist)\n",
    "2. [‚öîÔ∏è Adversarial attacks](#Ô∏èadv-att)\n",
    "3. [üõ°Ô∏è Robustness with Lipschitz-constrained networks](#lipschitz)\n",
    "\n",
    "\n",
    "**Package Requirements**\n",
    "\n",
    "`notebook`, `torch`, `torchvision`, `torchattacks`, `deel-torchlip`, `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üß† Train a standard neural network on MNIST <a id=\"train-mnist\"></a>\n",
    "\n",
    "In this first section, we will train a very simple CNN on MNIST. There is no exercice\n",
    "here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device & Hyperparameters\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "VAL_SPLIT = 0.1\n",
    "NUM_EPOCHS = 10\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load MNIST dataset\n",
    "\n",
    "We will:\n",
    "\n",
    "- Use the **MNIST** dataset (handwritten digits 0‚Äì9).\n",
    "- Split the original training set into:\n",
    "  - a **train** set\n",
    "  - a small **validation** set (to monitor training)\n",
    "\n",
    "We also create a **test** loader to evaluate the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders for MNIST\n",
    "def get_mnist_dataloaders(batch_size=BATCH_SIZE, val_split=VAL_SPLIT):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    trainval_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "    n_total = len(trainval_dataset)\n",
    "    n_val = int(n_total * val_split)\n",
    "    n_train = n_total - n_val\n",
    "\n",
    "    train_dataset, val_dataset = random_split(trainval_dataset, [n_train, n_val])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = get_mnist_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build a simple CNN network\n",
    "\n",
    "We define a very small convolutional neural network:\n",
    "\n",
    "- Two convolutional layers with ReLU and max pooling.\n",
    "- Two fully connected layers.\n",
    "\n",
    "This is not a state-of-the-art model, but it is good enough to reach high accuracy on\n",
    "MNIST and to demonstrate adversarial attacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small CNN Model\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 1x28x28 -> 32x28x28\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 32x14x14 -> 64x14x14\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # downsample by 2\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> 32 x 14 x 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> 64 x 7 x 7\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = SimpleCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training\n",
    "\n",
    "We define three helper functions:\n",
    "\n",
    "- `train_one_epoch` ‚Äì does one pass over the training data.\n",
    "- `evaluate` ‚Äì computes loss and accuracy on a given dataloader.\n",
    "- `train` ‚Äì trains on multiple epochs using the two previous helper functions. \n",
    "\n",
    "We will train only for a few epochs (e.g., 10 epochs).  \n",
    "The goal is just to get the model to a reasonable accuracy so that we can see meaningful\n",
    "adversarial examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Evaluation helper functions\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device):\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} \"\n",
    "            f\"| Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the standard model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Evaluate on Test Set\n",
    "\n",
    "Now we measure how well the trained model performs on unseen test images.\n",
    "We expect a high accuracy (above 98% usually), but the exact number is not critical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Evaluation\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Visualizing Logits for One Image\n",
    "\n",
    "The model outputs **logits**: raw scores for each class (0‚Äì9).  \n",
    "We will visualize these logits by sorting them in descending order.\n",
    "\n",
    "This helps us see how \"confident\" the model is for each class. We will see later that\n",
    "for Lipschitz-constrained networks, these logits are keys to guarantee the robustness\n",
    "of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot logits (sorted) with top-1 prediction highlighted\n",
    "\n",
    "MNIST_CLASS_NAMES = [str(i) for i in range(10)]\n",
    "\n",
    "\n",
    "def plot_sorted_logits(logits: torch.Tensor, class_names=MNIST_CLASS_NAMES, use_softmax=False):\n",
    "    \"\"\"\n",
    "    logits: 1D tensor of shape (num_classes,)\n",
    "    Plots logits sorted by value (descending) with class names on x-axis.\n",
    "    Top-1 predicted class is the first bar.\n",
    "    \"\"\"\n",
    "    if logits.dim() != 1:\n",
    "        raise ValueError(\"logits must be a 1D tensor for a single sample.\")\n",
    "\n",
    "    logits = logits.detach().cpu()\n",
    "    if use_softmax:\n",
    "        logits = F.softmax(logits, dim=0)\n",
    "    sorted_indices = torch.argsort(logits, descending=True)\n",
    "    sorted_logits = logits[sorted_indices]\n",
    "\n",
    "    labels = [class_names[i] for i in sorted_indices]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bars = plt.bar(range(len(sorted_logits)), sorted_logits)\n",
    "\n",
    "    # Optionally highlight the top-1 bar\n",
    "    bars[0].set_color(\"orange\")\n",
    "\n",
    "    plt.xticks(range(len(sorted_logits)), labels)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Logit value\")\n",
    "    plt.title(\"Logits sorted\" + (\" (Softmax)\" if use_softmax else \"\"))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_images, sample_labels = next(iter(test_loader))\n",
    "    sample_images, sample_labels = sample_images.to(device), sample_labels.to(device)\n",
    "    sample_logits = model(sample_images)\n",
    "\n",
    "# Plot single image\n",
    "idx = 7\n",
    "plt.imshow(sample_images[idx].cpu().squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"True Label: {sample_labels[idx].item()}, Predicted: {sample_logits[idx].argmax().item()}\")\n",
    "\n",
    "# Plot sorted logits on one test sample\n",
    "plot_sorted_logits(sample_logits[idx], MNIST_CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ‚öîÔ∏è Adversarial attacks <a id=\"adv-att\"></a>\n",
    "\n",
    "An adversarial attack in classification is the process of finding an image $x_{adv}$\n",
    "close to the original image $x$ such that the predicted class is different, i.e.\n",
    "$f(x_{adv})$ and $f(x)$ differs. It was shown that it is easy to build such an\n",
    "adversarial example on standard neural networks even with a very small added noise. In\n",
    "the following, we suppose that the original image is correctly classified, i.e. the\n",
    "attack tries to predict a class different from the true label.\n",
    "\n",
    "Related to the sorted logits plot above, the idea is to find a small perturbation on the\n",
    "input in order to change the highest logit. \n",
    "\n",
    "In the following, we will:\n",
    "1. Implement and run the simple FGM method, in the untargeted setup.\n",
    "2. Implement and run the same attack but with a targeted objective.\n",
    "3. Use a more sophisticated attack with the `torchattacks` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Untargeted FGM attack\n",
    "\n",
    "Here, we want to implement the untargeted FGM attack, defined as:\n",
    "\n",
    "$$x_{adv} = x + \\epsilon \\frac{\\nabla_{x} l(f_\\theta(x), y)}{\\| \\nabla_{x}\n",
    "l(f_\\theta(x), y) \\|}.$$\n",
    "\n",
    "$\\epsilon$ defines the budget of the attack and controls how strong the attack is. Note\n",
    "that the distance between $x_{adv}$ and $x$ (in $\\ell_2$ norm) is $\\epsilon$. The FGM\n",
    "attack simply goes in the direction of the gradient to increase the loss for the true\n",
    "label. It is highly related to the L2-PGD attack with a single iteration.\n",
    "\n",
    "**Exercise 2.1.1**\n",
    "\n",
    "Complete the following code for the untargeted FGM attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the untargeted FGM Attack\n",
    "\n",
    "\n",
    "def fgm_attack_untargeted(model, images, labels, epsilon, loss_fn=None):\n",
    "    \"\"\"\n",
    "    Untargeted FGM.\n",
    "\n",
    "    Args:\n",
    "        images: (N, C, H, W) tensor, normalized as in training\n",
    "        labels: (N,) tensor for true labels\n",
    "        epsilon: attack budget (same space as normalized images)\n",
    "        loss_fn: loss function to maximize (default: CrossEntropyLoss)\n",
    "\n",
    "    Returns:\n",
    "        adv_images: (N, C, H, W) tensor of FGM adversarial samples\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    loss_fn = loss_fn or nn.CrossEntropyLoss()\n",
    "\n",
    "    images = images.clone().detach().to(device)\n",
    "    labels = labels.to(device)\n",
    "    images.requires_grad = True\n",
    "\n",
    "    outputs = model(images)\n",
    "\n",
    "    # TODO: compute gradient of the loss and create adversarial examples\n",
    "    ...\n",
    "\n",
    "    adv_images = torch.clamp(adv_images, 0, 1)\n",
    "\n",
    "    return adv_images.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FGM attack on the first batch\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "adv_images_untargeted = fgm_attack_untargeted(model, images, labels, epsilon=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adversarial_examples(model, original, adversarial):\n",
    "    \"\"\"\n",
    "    Displays the original and adversarial images, and their difference.\n",
    "\n",
    "    Args:\n",
    "        model: the trained model (used to get predictions)\n",
    "        original: (1, H, W) tensor\n",
    "        adversarial: (1, H, W) tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Get prediction of original and adversarial\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        orig_pred = model(original.unsqueeze(0).to(device)).argmax(dim=1).item()\n",
    "        adv_pred = model(adversarial.unsqueeze(0).to(device)).argmax(dim=1).item()\n",
    "\n",
    "    original = original.cpu().squeeze()  # (1, H, W) -> (H, W)\n",
    "    adversarial = adversarial.cpu().squeeze()  # (1, H, W) -> (H, W)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(f\"Original Image (pred: {orig_pred})\")\n",
    "    plt.imshow(original, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"Adversarial Image (pred: {adv_pred})\")\n",
    "    plt.imshow(adversarial, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    diff_norm = torch.norm(adversarial - original)\n",
    "    plt.title(f\"Difference (epsilon={diff_norm:.2f})\")\n",
    "    diff_pos = F.relu(adversarial - original)\n",
    "    diff_neg = F.relu(original - adversarial)\n",
    "    difference = torch.stack([diff_neg, diff_pos, torch.zeros_like(diff_pos)], dim=-1)\n",
    "    difference /= difference.abs().max()\n",
    "    plt.imshow(difference, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and adversarial images\n",
    "\n",
    "idx = 7\n",
    "plot_adversarial_examples(model, images[idx], adv_images_untargeted[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sorted logits for original and adversarial images\n",
    "\n",
    "plot_sorted_logits(model(images[idx].unsqueeze(0).to(device)).squeeze(), MNIST_CLASS_NAMES)\n",
    "plot_sorted_logits(model(adv_images_untargeted[idx].unsqueeze(0).to(device)).squeeze(), MNIST_CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1.2**\n",
    "\n",
    "We want to find (approximately) the minimum budget $\\epsilon$ to change the original\n",
    "class. We will perform successive attacks with different values of $\\epsilon$ and find\n",
    "when the attack succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change epsilon budget to observe when the attack becomes successful\n",
    "\n",
    "epsilons = ...\n",
    "\n",
    "for eps in epsilons:\n",
    "    adv_images_untargeted = ...\n",
    "\n",
    "    adv_preds = ...\n",
    "    diffs = ...\n",
    "\n",
    "    if adv_preds[idx].item() != labels[idx].item():\n",
    "        print(f\"‚úÖ Attack succeeded at epsilon = {diffs:.4f}  (set budget = {eps})\")\n",
    "    else:\n",
    "        print(f\"‚ùå Attack failed at epsilon = {diffs:.4f}  (set budget = {eps})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Targeted FGM attack\n",
    "\n",
    "Previously, the untargeted attack aims at changing the predicted class, whatever the new\n",
    "prediction. In the targeted setting, we want to attack towards a specific class.\n",
    "Depending on the target, this attack can require more budget to succeed.\n",
    "\n",
    "The targeted adversarial sample $x_{adv}$ is:\n",
    "\n",
    "$$x_{adv} = x - \\epsilon \\frac{\\nabla_{x} l(f_\\theta(x), y^t)}{\\| \\nabla_{x}\n",
    "l(f_\\theta(x), y^t) \\|}$$\n",
    "\n",
    "with $y^t$ being the targeted class. Note the **minus sign**: we are moving the input in\n",
    "the direction that makes the model more confident in the target class, i.e minimizing\n",
    "the loss considering the target class.\n",
    "\n",
    "**Exercise 2.2.1**\n",
    "\n",
    "Implement the targeted FGM attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targeted FGM Attack\n",
    "\n",
    "\n",
    "def fgm_attack_targeted(model, images, target_labels, epsilon, loss_fn=None):\n",
    "    \"\"\"\n",
    "    Targeted FGM\n",
    "\n",
    "    Args:\n",
    "        images: (N, C, H, W) tensor, normalized as in training\n",
    "        target_labels: (N,) tensor of desired target classes\n",
    "        epsilon: attack budget (same space as normalized images)\n",
    "        loss_fn: loss function to minimize (default: CrossEntropyLoss)\n",
    "\n",
    "    Returns:\n",
    "        adv_images: (N, C, H, W) tensor of targeted FGM adversarial samples\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_fn = loss_fn or nn.CrossEntropyLoss()\n",
    "\n",
    "    images = images.clone().detach().to(device)\n",
    "    target_labels = target_labels.to(device)\n",
    "    images.requires_grad = True\n",
    "\n",
    "    outputs = model(images)\n",
    "\n",
    "    # TODO: compute gradient of the loss and create adversarial examples\n",
    "    ...\n",
    "\n",
    "    adv_images = torch.clamp(adv_images, 0, 1)\n",
    "\n",
    "    return adv_images.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run targeted FGM attack\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "target_labels = torch.ones_like(labels) * 2  # targets all set to class '8'\n",
    "adv_images_targeted = fgm_attack_targeted(model, images, target_labels, epsilon=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and targeted adversarial images\n",
    "\n",
    "idx = 7\n",
    "plot_adversarial_examples(model, images[idx], adv_images_targeted[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sorted logits for original and adversarial images\n",
    "\n",
    "plot_sorted_logits(model(images[idx].unsqueeze(0).to(device)).squeeze(), MNIST_CLASS_NAMES)\n",
    "plot_sorted_logits(model(adv_images_targeted[idx].unsqueeze(0).to(device)).squeeze(), MNIST_CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run more complex attacks\n",
    "\n",
    "The FGM attack implemented above is a very simple way to find an adversarial example.\n",
    "Many attacks exist and perform much better than FGM, e.g. PGD, Carlini&Wagner,\n",
    "Auto-Attack, etc.\n",
    "\n",
    "There are several Python libraries to run adversarial attacks: \n",
    "\n",
    "- [ART toolbox](https://adversarial-robustness-toolbox.readthedocs.io/en/latest/)\n",
    "- [foolbox](https://foolbox.jonasrauber.de/)\n",
    "- [Adversarial Library](https://github.com/jeromerony/adversarial-library)\n",
    "- [Torchattacks](https://github.com/Harry24k/adversarial-attacks-pytorch)\n",
    "\n",
    "\n",
    "We will use the `torchattacks` library that implements several well-known attacks.\n",
    "\n",
    "\n",
    "**Exercise 2.3.1**\n",
    "\n",
    "Run APGD attack on the first batch of test images. Be careful with `torchattacks`: if\n",
    "the attack fails and no adversarial image is found within the budget, `torchattacks`\n",
    "returns the original image (instead of an adversarial one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run APGD with torchattacks\n",
    "\n",
    "from torchattacks import APGD\n",
    "\n",
    "# TODO : create APGD attack instance, and run it on the first batch of test images\n",
    "...\n",
    "\n",
    "adv_images_apgd = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 7\n",
    "plot_adversarial_examples(model, images[idx], adv_images_apgd[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3.2**\n",
    "\n",
    "Compare performance with the FGM attack. Which attack performs best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison between FGM and APGD\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go further, you can try other attacks, like AutoAttack which is a combination of\n",
    "state-of-the-art attacks. AutoAttack performs very well but is costly due to the\n",
    "evaluations of multiple attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üõ°Ô∏è Robustness with Lipschitz-constrained networks <a id=\"lipschitz\"></a>\n",
    "\n",
    "In this last section, we will see how Lipschitz-constrained networks can be used to\n",
    "be robust to adversarial attacks and to certify the robustness.\n",
    "\n",
    "### 3.1 Build and train a Lipschitz-constrained network\n",
    "\n",
    "First, we will build and train a simple Lipschitz-constrained network, with the same\n",
    "architecture as the unconstrained network above.\n",
    "\n",
    "Recall that the loss is the keystone of the accuracy/robustness trade-off. The\n",
    "hyperparameter allows to tune the robustness of the model. Here, using the\n",
    "Tau-Cross-Entropy loss, the model is more robust for small temperatures `tau`.\n",
    "\n",
    "\n",
    "**Exercise 3.1.1**\n",
    "\n",
    "Build a LipNet using the `torchlip` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Lipschitz CNN Model\n",
    "\n",
    "from deel.torchlip import ...\n",
    "\n",
    "\n",
    "class LipCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = ...\n",
    "        self.conv2 = ...\n",
    "        self.pool = ...\n",
    "        self.fc1 = ...\n",
    "        self.fc2 = ...\n",
    "        self.activation = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x)))  # -> 32 x 14 x 14\n",
    "        x = self.pool(self.activation(self.conv2(x)))  # -> 64 x 7 x 7\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.activation(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "lip_model = LipCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1.2**\n",
    "\n",
    "Train the LipNet with a Tau-Cross-Entropy. Set the temperature to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training LipCNN with Tau-Cross-Entropy Loss\n",
    "\n",
    "from deel.torchlip import TauCrossEntropyLoss\n",
    "\n",
    "# TODO: Set the loss\n",
    "criterion = ...\n",
    "optimizer = torch.optim.Adam(lip_model.parameters(), lr=LR)\n",
    "\n",
    "train(lip_model, train_loader, val_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Evaluation\n",
    "\n",
    "test_loss, test_acc = evaluate(lip_model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot image and sorted logits for LipCNN\n",
    "\n",
    "lip_model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_images, sample_labels = next(iter(test_loader))\n",
    "    sample_images, sample_labels = sample_images.to(device), sample_labels.to(device)\n",
    "    sample_logits = lip_model(sample_images)\n",
    "\n",
    "# Plot a single image\n",
    "idx = 7\n",
    "plt.imshow(sample_images[idx].cpu().squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"True Label: {sample_labels[idx].item()}, Predicted: {sample_logits[idx].argmax().item()}\")\n",
    "\n",
    "# Plot sorted logits\n",
    "plot_sorted_logits(sample_logits[idx], MNIST_CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compute robustness certificates\n",
    "\n",
    "Thanks to the knowledge of the Lipschitz constant, we can guarantee that there is no\n",
    "attack with a budget lower than the certificate $\\mathcal{M}$ that can change the\n",
    "prediction of the original image $x$. This certificate is defined as:\n",
    "\n",
    "$$ \\mathcal{M}(x) = \\frac{f_\\theta(x)_{top1} - f_\\theta(x)_{top2}}{\\sqrt{2}} $$\n",
    "\n",
    "This certificate is based on the difference between the two highest logits (recall the figure above where we plot the sorted logits).\n",
    "\n",
    "\n",
    "**Exercise 3.2.1**\n",
    "\n",
    "Complete the function that builds the certificates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Lipschitz Certificates\n",
    "\n",
    "\n",
    "def compute_certificates(lip_model, images):\n",
    "    \"\"\"\n",
    "    Computes Lipschitz certificates for the given images.\n",
    "\n",
    "    Args:\n",
    "        lip_model: the trained Lipschitz model\n",
    "        images: (N, C, H, W) tensor of input images\n",
    "\n",
    "    Returns:\n",
    "        a (N,) tensor of certificates\n",
    "    \"\"\"\n",
    "\n",
    "    lip_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = lip_model(images.to(device))\n",
    "        sorted_logits, _ = ...\n",
    "        margins = ...\n",
    "        certificates = ...\n",
    "    return certificates.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute certificates for a batch of test images\n",
    "\n",
    "certificates = compute_certificates(lip_model, images)\n",
    "print(\"Certificates:\\n\", certificates)\n",
    "print(\"Certificate for image idx =\", idx, \":\", certificates[idx].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run adversarial attacks on the LipNet\n",
    "\n",
    "We can compare the empirical robustness of our LipNet with the unconstrained network.\n",
    "\n",
    "**Exercise 3.3.1**\n",
    "\n",
    "Run untargeted FGM attack on the LipNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FGM attack on the first batch\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "adv_images_lip_untargeted = ...\n",
    "\n",
    "idx = 7\n",
    "plot_adversarial_examples(lip_model, images[idx], adv_images_lip_untargeted[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3.2**\n",
    "\n",
    "Find (approximately) the budget required to perturb the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change epsilon budget to observe when the attack becomes successful\n",
    "\n",
    "epsilons = ...\n",
    "for eps in epsilons:\n",
    "    adv_images_lip_untargeted = ...\n",
    "\n",
    "    adv_preds = ...\n",
    "    diffs = ...\n",
    "\n",
    "    if adv_preds[idx].item() != labels[idx].item():\n",
    "        print(f\"‚úÖ Attack succeeded at epsilon = {diffs:.4f}  (set budget = {eps})\")\n",
    "    else:\n",
    "        print(f\"‚ùå Attack failed at epsilon = {diffs:.4f}  (set budget = {eps})\")\n",
    "\n",
    "    # plot_adversarial_examples(model, images[idx], adv_images_lip_untargeted[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3.2**\n",
    "\n",
    "Run APGD attack on the LipNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run APGD on LipCNN\n",
    "\n",
    "# TODO : create APGD attack instance, and run it on the first batch of test images\n",
    "...\n",
    "\n",
    "\n",
    "adv_images_apgd_lip = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 7\n",
    "plot_adversarial_examples(lip_model, images[idx], adv_images_apgd_lip[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3.3**\n",
    "\n",
    "1. Compare the empirical budget epsilon and the robustness certificate.\n",
    "2. Compare the empirical robustness between the standard and the Lipschitz network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Go further: Train and evaluate with different temperatures\n",
    "\n",
    "You can re-run the sections 3.1 to 3.3 with a different temperature `tau`. We expect\n",
    "that for smaller temperatures, the robustness of the LipNet is increased, with a small\n",
    "drop in accuracy. The temperature is an important parameter to move on the Pareto\n",
    "frontier of the accuracy-robustness curve.\n",
    "\n",
    "\n",
    "**Exercise 3.4.1**\n",
    "\n",
    "Complete the following code to train a new LipNet with a lower temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_model_tau1 = LipCNN().to(device)\n",
    "\n",
    "criterion = ...\n",
    "optimizer = torch.optim.Adam(lip_model_tau1.parameters(), lr=LR)\n",
    "\n",
    "train(lip_model_tau1, train_loader, val_loader, optimizer, criterion, device)\n",
    "\n",
    "test_loss, test_acc = evaluate(lip_model_tau1, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_model_tau1.eval()\n",
    "with torch.no_grad():\n",
    "    sample_images, sample_labels = next(iter(test_loader))\n",
    "    sample_images, sample_labels = sample_images.to(device), sample_labels.to(device)\n",
    "    sample_logits = lip_model_tau1(sample_images)\n",
    "\n",
    "# Plot a single image\n",
    "idx = 7\n",
    "plt.imshow(sample_images[idx].cpu().squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"True Label: {sample_labels[idx].item()}, Predicted: {sample_logits[idx].argmax().item()}\")\n",
    "\n",
    "# Plot sorted logits\n",
    "plot_sorted_logits(sample_logits[idx], MNIST_CLASS_NAMES)\n",
    "\n",
    "certificates = compute_certificates(lip_model_tau1, images)\n",
    "print(\"Certificates:\\n\", certificates)\n",
    "\n",
    "\n",
    "# Run APGD on LipCNN\n",
    "...\n",
    "\n",
    "adv_images_apgd_lip = ...\n",
    "idx = 7\n",
    "plot_adversarial_examples(lip_model_tau1, images[idx], adv_images_apgd_lip[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Certified Robust accuracy (CRA) on the test set\n",
    "\n",
    "In this tutorial, we focus on the attack and robustness of a single image: we attacked\n",
    "with a given budget and measured if it was succesful, and we computed the certificate of\n",
    "an image (for LipNets).\n",
    "\n",
    "Generally, we measure the performance of an attack or a defense on a whole test set. For\n",
    "attacks, we measure the empirical robust accuracy at a given budget epsilon. In other\n",
    "terms, we compute how many images of the test set can be misclassified by the attacker.\n",
    "\n",
    "For LipNets, we can compute the Certified Robust Accuracy (CRA) which corresponds to the\n",
    "percentage of certificates on the test set that are above a given budget epsilon. It\n",
    "corresponds to a lower bound of the empirical robust accuracy measured by an adversarial\n",
    "attack.\n",
    "\n",
    "**Exercise 4.1**\n",
    "\n",
    "1. Compute the empirical robust accuracy on the test set using the attack and the budget\n",
    "   of your choice.\n",
    "2. Compute the CRA on the test set for the same budget."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
